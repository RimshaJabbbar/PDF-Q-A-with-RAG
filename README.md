# PDF-Q-A-with-RAG
ğŸ“„ RAG-based PDF Q&amp;A app with chat history, built using Streamlit, LangChain, Groq LLM, and HuggingFace embeddings. Upload PDFs, ask questions, and get context-aware answers with persistent chat sessions.

This project is an interactive Retrieval-Augmented Generation (RAG) application that allows users to upload multiple PDF documents and query them through a conversational interface.

Built with Streamlit for the frontend and LangChain for orchestration, the app leverages:

Groq LLM (gemma2-9b-it) for natural language question answering.

HuggingFace Embeddings (all-MiniLM-L6-v2) for semantic document retrieval.

ChromaDB as a vector store for efficient similarity search.

History-aware retriever to refine answers using previous chat context.

âœ¨ Features

ğŸ“¤ Multi-PDF Upload â€“ Upload one or more PDF documents for contextual querying.

ğŸ” RAG Pipeline â€“ Retrieves relevant chunks from PDFs before answering.

ğŸ§  Chat History Memory â€“ Maintains session-based conversational context.

ğŸ”‘ Groq API Integration â€“ Securely use your own Groq API key.

âš¡ Fast Embeddings â€“ Uses HuggingFace embeddings with CPU fallback.

ğŸ–¥ Streamlit UI â€“ Clean and interactive interface for Q&A and chat history exploration.

ğŸš€ How It Works

Upload your PDF(s).

The app loads and splits them into text chunks.

Embeddings are generated and stored in ChromaDB.

Ask any question in natural language.

The retriever finds relevant chunks, and the LLM answers using both documents + chat history.

ğŸ› ï¸ Tech Stack

Python 3.9+

Streamlit (frontend & UI)

LangChain (retrievers, prompts, memory, chaining)

Groq LLM API (for reasoning/answer generation)

HuggingFace Sentence Transformers (text embeddings)

ChromaDB (vector database for document retrieval)
